{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run them to download ckpts, configs and datasets.\n",
    "\n",
    "# !mkdir -p logs/2020-11-09T13-31-51_sflckr/checkpoints\n",
    "# !wget 'https://heibox.uni-heidelberg.de/d/73487ab6e5314cb5adba/files/?p=%2Fcheckpoints%2Flast.ckpt&dl=1' -O 'logs/2020-11-09T13-31-51_sflckr/checkpoints/last.ckpt'\n",
    "# !mkdir logs/2020-11-09T13-31-51_sflckr/configs\n",
    "# !wget 'https://heibox.uni-heidelberg.de/d/73487ab6e5314cb5adba/files/?p=%2Fconfigs%2F2020-11-09T13-31-51-project.yaml&dl=1' -O 'logs/2020-11-09T13-31-51_sflckr/configs/2020-11-09T13-31-51-project.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\".\")\n",
    "project_path = \"/home/lyk/Projects/taming-transformers\" # Change this!!!\n",
    "config_path = \"logs/2020-11-09T13-31-51_sflckr/configs/2020-11-09T13-31-51-project.yaml\"\n",
    "ckpt_path = \"logs/2020-11-09T13-31-51_sflckr/checkpoints/last.ckpt\"\n",
    "\n",
    "# image_directory_path = f\"{project_path}/data/coco_images\"\n",
    "image_directory_path = f\"{project_path}/my_data/flickr30k_images\"\n",
    "# image_directory_path = f\"{project_path}/my_data/test_images\"\n",
    "\n",
    "\n",
    "file_path_to_save = f\"{project_path}/outputs/result.pth\"\n",
    "# Check if CUDA is available\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "target_image_size = 256  # Optional: Resize all images to (H, W) while H=W\n",
    "batch_size=32\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:\n",
      "  params:\n",
      "    batch_size: 1\n",
      "    validation:\n",
      "      target: taming.data.sflckr.Examples\n",
      "  target: main.DataModuleFromConfig\n",
      "model:\n",
      "  base_learning_rate: 4.5e-06\n",
      "  params:\n",
      "    cond_stage_config:\n",
      "      params:\n",
      "        ddconfig:\n",
      "          attn_resolutions:\n",
      "          - 16\n",
      "          ch: 128\n",
      "          ch_mult:\n",
      "          - 1\n",
      "          - 1\n",
      "          - 2\n",
      "          - 2\n",
      "          - 4\n",
      "          double_z: false\n",
      "          dropout: 0.0\n",
      "          in_channels: 182\n",
      "          num_res_blocks: 2\n",
      "          out_ch: 182\n",
      "          resolution: 256\n",
      "          z_channels: 256\n",
      "        embed_dim: 256\n",
      "        image_key: segmentation\n",
      "        lossconfig:\n",
      "          target: taming.modules.losses.DummyLoss\n",
      "        n_embed: 1024\n",
      "      target: taming.models.vqgan.VQModel\n",
      "    cond_stage_key: segmentation\n",
      "    first_stage_config:\n",
      "      params:\n",
      "        ddconfig:\n",
      "          attn_resolutions:\n",
      "          - 16\n",
      "          ch: 128\n",
      "          ch_mult:\n",
      "          - 1\n",
      "          - 1\n",
      "          - 2\n",
      "          - 2\n",
      "          - 4\n",
      "          double_z: false\n",
      "          dropout: 0.0\n",
      "          in_channels: 3\n",
      "          num_res_blocks: 2\n",
      "          out_ch: 3\n",
      "          resolution: 256\n",
      "          z_channels: 256\n",
      "        embed_dim: 256\n",
      "        lossconfig:\n",
      "          target: taming.modules.losses.DummyLoss\n",
      "        n_embed: 1024\n",
      "      target: taming.models.vqgan.VQModel\n",
      "    first_stage_key: image\n",
      "    transformer_config:\n",
      "      params:\n",
      "        block_size: 512\n",
      "        n_embd: 1024\n",
      "        n_head: 16\n",
      "        n_layer: 24\n",
      "        vocab_size: 1024\n",
      "      target: taming.modules.transformer.mingpt.GPT\n",
      "  target: taming.models.cond_transformer.Net2NetTransformer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config = OmegaConf.load(config_path)\n",
    "import yaml\n",
    "print(yaml.dump(OmegaConf.to_container(config)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the mdoel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with z of shape (1, 256, 16, 16) = 65536 dimensions.\n",
      "Working with z of shape (1, 256, 16, 16) = 65536 dimensions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyk/miniconda3/envs/taming/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from taming.models.cond_transformer import Net2NetTransformer\n",
    "model = Net2NetTransformer(**config.model.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net2NetTransformer(\n",
       "  (first_stage_model): VQModel(\n",
       "    (encoder): Encoder(\n",
       "      (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (down): ModuleList(\n",
       "        (0-1): 2 x Module(\n",
       "          (block): ModuleList(\n",
       "            (0-1): 2 x ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (attn): ModuleList()\n",
       "          (downsample): Downsample(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "          )\n",
       "        )\n",
       "        (2): Module(\n",
       "          (block): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nin_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (1): ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (attn): ModuleList()\n",
       "          (downsample): Downsample(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
       "          )\n",
       "        )\n",
       "        (3): Module(\n",
       "          (block): ModuleList(\n",
       "            (0-1): 2 x ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (attn): ModuleList()\n",
       "          (downsample): Downsample(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
       "          )\n",
       "        )\n",
       "        (4): Module(\n",
       "          (block): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nin_shortcut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (1): ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (attn): ModuleList(\n",
       "            (0-1): 2 x AttnBlock(\n",
       "              (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (mid): Module(\n",
       "        (block_1): ResnetBlock(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (attn_1): AttnBlock(\n",
       "          (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (block_2): ResnetBlock(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "      (conv_out): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (decoder): Decoder(\n",
       "      (conv_in): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (mid): Module(\n",
       "        (block_1): ResnetBlock(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (attn_1): AttnBlock(\n",
       "          (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (block_2): ResnetBlock(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (up): ModuleList(\n",
       "        (0): Module(\n",
       "          (block): ModuleList(\n",
       "            (0-2): 3 x ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (attn): ModuleList()\n",
       "        )\n",
       "        (1): Module(\n",
       "          (block): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nin_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (1-2): 2 x ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (attn): ModuleList()\n",
       "          (upsample): Upsample(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (2): Module(\n",
       "          (block): ModuleList(\n",
       "            (0-2): 3 x ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (attn): ModuleList()\n",
       "          (upsample): Upsample(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (3): Module(\n",
       "          (block): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nin_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (1-2): 2 x ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (attn): ModuleList()\n",
       "          (upsample): Upsample(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (4): Module(\n",
       "          (block): ModuleList(\n",
       "            (0-2): 3 x ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (attn): ModuleList(\n",
       "            (0-2): 3 x AttnBlock(\n",
       "              (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (upsample): Upsample(\n",
       "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "      (conv_out): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (loss): DummyLoss()\n",
       "    (quantize): VectorQuantizer2(\n",
       "      (embedding): Embedding(1024, 256)\n",
       "    )\n",
       "    (quant_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (post_quant_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (cond_stage_model): VQModel(\n",
       "    (encoder): Encoder(\n",
       "      (conv_in): Conv2d(182, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (down): ModuleList(\n",
       "        (0-1): 2 x Module(\n",
       "          (block): ModuleList(\n",
       "            (0-1): 2 x ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (attn): ModuleList()\n",
       "          (downsample): Downsample(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "          )\n",
       "        )\n",
       "        (2): Module(\n",
       "          (block): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nin_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (1): ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (attn): ModuleList()\n",
       "          (downsample): Downsample(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
       "          )\n",
       "        )\n",
       "        (3): Module(\n",
       "          (block): ModuleList(\n",
       "            (0-1): 2 x ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (attn): ModuleList()\n",
       "          (downsample): Downsample(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
       "          )\n",
       "        )\n",
       "        (4): Module(\n",
       "          (block): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nin_shortcut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (1): ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (attn): ModuleList(\n",
       "            (0-1): 2 x AttnBlock(\n",
       "              (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (mid): Module(\n",
       "        (block_1): ResnetBlock(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (attn_1): AttnBlock(\n",
       "          (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (block_2): ResnetBlock(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "      (conv_out): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (decoder): Decoder(\n",
       "      (conv_in): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (mid): Module(\n",
       "        (block_1): ResnetBlock(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (attn_1): AttnBlock(\n",
       "          (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (block_2): ResnetBlock(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (up): ModuleList(\n",
       "        (0): Module(\n",
       "          (block): ModuleList(\n",
       "            (0-2): 3 x ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (attn): ModuleList()\n",
       "        )\n",
       "        (1): Module(\n",
       "          (block): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nin_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (1-2): 2 x ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (attn): ModuleList()\n",
       "          (upsample): Upsample(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (2): Module(\n",
       "          (block): ModuleList(\n",
       "            (0-2): 3 x ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (attn): ModuleList()\n",
       "          (upsample): Upsample(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (3): Module(\n",
       "          (block): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nin_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (1-2): 2 x ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (attn): ModuleList()\n",
       "          (upsample): Upsample(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (4): Module(\n",
       "          (block): ModuleList(\n",
       "            (0-2): 3 x ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (attn): ModuleList(\n",
       "            (0-2): 3 x AttnBlock(\n",
       "              (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (upsample): Upsample(\n",
       "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "      (conv_out): Conv2d(128, 182, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (loss): DummyLoss()\n",
       "    (quantize): VectorQuantizer2(\n",
       "      (embedding): Embedding(1024, 256)\n",
       "    )\n",
       "    (quant_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (post_quant_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (permuter): Identity()\n",
       "  (transformer): GPT(\n",
       "    (tok_emb): Embedding(1024, 1024)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (blocks): Sequential(\n",
       "      (0): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (12): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (13): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (14): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (15): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (16): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (17): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (18): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (19): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (20): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (21): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (22): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (23): Block(\n",
       "        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (head): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd = torch.load(ckpt_path, map_location=\"cpu\")[\"state_dict\"]\n",
    "missing, unexpected = model.load_state_dict(sd, strict=False)\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x788d6c2c7850>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 3587077732.jpg (size: 214x500)\n",
      "Deleted 3179833180.jpg (size: 500x246)\n",
      "Deleted 2574824768.jpg (size: 500x189)\n",
      "Deleted 4612587813.jpg (size: 500x249)\n",
      "Deleted 247874544.jpg (size: 254x500)\n",
      "Deleted 3381747300.jpg (size: 216x500)\n",
      "Deleted 4752463128.jpg (size: 500x191)\n",
      "Deleted 4614283206.jpg (size: 500x238)\n",
      "Deleted 473113241.jpg (size: 244x500)\n",
      "Deleted 4903277818.jpg (size: 500x240)\n",
      "Deleted 437917001.jpg (size: 500x252)\n",
      "Deleted 3683644335.jpg (size: 500x192)\n",
      "Deleted 23015660.jpg (size: 253x500)\n",
      "Deleted 3380643902.jpg (size: 500x239)\n",
      "Deleted 4922641559.jpg (size: 207x500)\n",
      "Deleted 7992914279.jpg (size: 500x233)\n",
      "Deleted 3331797838.jpg (size: 500x219)\n",
      "Deleted 7764955218.jpg (size: 500x246)\n",
      "Deleted 3143032614.jpg (size: 500x197)\n",
      "Deleted 422763475.jpg (size: 244x500)\n",
      "Deleted 316577571.jpg (size: 500x253)\n",
      "Deleted 4729067306.jpg (size: 500x254)\n",
      "Deleted 6854823446.jpg (size: 500x225)\n",
      "Deleted 5989324335.jpg (size: 500x231)\n",
      "Deleted 1295671216.jpg (size: 200x500)\n",
      "Deleted 3091921457.jpg (size: 400x251)\n",
      "Deleted 2549203985.jpg (size: 500x234)\n",
      "Deleted 2792212974.jpg (size: 500x253)\n",
      "Deleted 4887879873.jpg (size: 500x185)\n",
      "Deleted 2896640216.jpg (size: 500x249)\n",
      "Deleted 3590593467.jpg (size: 500x240)\n",
      "Deleted 3195188609.jpg (size: 500x201)\n",
      "Deleted 5319238490.jpg (size: 500x201)\n",
      "Deleted 4873901110.jpg (size: 500x200)\n",
      "Deleted 4612828257.jpg (size: 500x224)\n",
      "Deleted 2562347802.jpg (size: 342x244)\n",
      "Deleted 178045.jpg (size: 500x188)\n",
      "Deleted 3276448136.jpg (size: 500x229)\n",
      "Deleted 406901451.jpg (size: 500x237)\n",
      "Deleted 2646246120.jpg (size: 500x198)\n",
      "Deleted 4110261078.jpg (size: 500x217)\n",
      "Deleted 1616085092.jpg (size: 500x238)\n",
      "Deleted 3221769307.jpg (size: 500x251)\n",
      "Deleted 4895859008.jpg (size: 500x216)\n",
      "Deleted 4652878735.jpg (size: 500x228)\n",
      "Deleted 3229821595.jpg (size: 500x176)\n",
      "Deleted 3671957239.jpg (size: 500x253)\n",
      "Deleted 2197552440.jpg (size: 500x249)\n",
      "Deleted 3646820231.jpg (size: 500x253)\n",
      "Deleted 4780479274.jpg (size: 500x186)\n",
      "Deleted 2766291711.jpg (size: 500x207)\n",
      "Deleted 4833088018.jpg (size: 500x213)\n",
      "Deleted 6882420474.jpg (size: 500x250)\n",
      "Deleted 2439384468.jpg (size: 500x214)\n",
      "Deleted 3647750811.jpg (size: 500x248)\n",
      "Deleted 268654674.jpg (size: 500x244)\n",
      "Deleted 4346525054.jpg (size: 500x159)\n",
      "Deleted 2252613928.jpg (size: 500x203)\n",
      "Deleted 4768114318.jpg (size: 500x217)\n",
      "Deleted 615025092.jpg (size: 500x224)\n",
      "Deleted 4642842928.jpg (size: 500x225)\n",
      "Deleted 4959071809.jpg (size: 500x244)\n",
      "Deleted 4859528111.jpg (size: 500x250)\n",
      "Deleted 7400312926.jpg (size: 500x199)\n",
      "Deleted 3645809088.jpg (size: 500x231)\n",
      "Deleted 473482566.jpg (size: 230x500)\n",
      "Deleted 3039209547.jpg (size: 164x500)\n",
      "Deleted 7173096497.jpg (size: 235x500)\n",
      "Deleted 3014773357.jpg (size: 500x148)\n",
      "Deleted 4656231658.jpg (size: 500x242)\n",
      "Deleted 4971580248.jpg (size: 246x500)\n",
      "Deleted 1001545525.jpg (size: 500x230)\n",
      "Deleted 3053785363.jpg (size: 500x218)\n",
      "Deleted 4679633895.jpg (size: 500x227)\n",
      "Deleted 1448511770.jpg (size: 500x252)\n",
      "Deleted 3403860799.jpg (size: 298x221)\n",
      "Deleted 3553056438.jpg (size: 246x400)\n",
      "Deleted 3197791645.jpg (size: 320x240)\n",
      "Deleted 4979962304.jpg (size: 500x184)\n",
      "Deleted 2402088539.jpg (size: 240x320)\n",
      "Deleted 3004291289.jpg (size: 212x500)\n",
      "Deleted 2949337912.jpg (size: 500x187)\n",
      "Deleted 7672048656.jpg (size: 242x500)\n",
      "Deleted 3654103642.jpg (size: 500x195)\n",
      "Deleted 4754011554.jpg (size: 500x250)\n",
      "Deleted 1213336750.jpg (size: 500x245)\n",
      "Deleted 7320251226.jpg (size: 500x250)\n",
      "Deleted 6493391567.jpg (size: 500x186)\n",
      "Deleted 6293805900.jpg (size: 500x243)\n",
      "Deleted 6620094641.jpg (size: 226x262)\n",
      "Deleted 4959665796.jpg (size: 500x245)\n",
      "Deleted 7035417199.jpg (size: 500x255)\n",
      "Deleted 6493393907.jpg (size: 500x195)\n",
      "Deleted 3322443827.jpg (size: 251x500)\n",
      "Deleted 2928835996.jpg (size: 500x249)\n",
      "Deleted 2629027962.jpg (size: 288x191)\n",
      "Deleted 2762594886.jpg (size: 500x182)\n",
      "Deleted 357191373.jpg (size: 500x253)\n",
      "Deleted 2892177947.jpg (size: 500x238)\n",
      "Deleted 6659698.jpg (size: 240x320)\n",
      "Deleted 4710499155.jpg (size: 500x183)\n",
      "Deleted 4941418868.jpg (size: 500x250)\n",
      "Deleted 4717486227.jpg (size: 237x500)\n",
      "Deleted 3646120271.jpg (size: 500x250)\n",
      "Deleted 4388402668.jpg (size: 240x320)\n",
      "Deleted 3046429283.jpg (size: 182x500)\n",
      "Deleted 7974529704.jpg (size: 500x242)\n",
      "Deleted 1484711469.jpg (size: 500x157)\n",
      "Deleted 6942665770.jpg (size: 320x240)\n",
      "Deleted 2685788323.jpg (size: 253x500)\n",
      "Deleted 4929731785.jpg (size: 500x250)\n",
      "Deleted 4690240999.jpg (size: 500x199)\n",
      "Deleted 1620397000.jpg (size: 500x241)\n",
      "Deleted 7117181861.jpg (size: 500x244)\n",
      "Deleted 1240297429.jpg (size: 500x167)\n",
      "Deleted 7075953327.jpg (size: 500x195)\n",
      "Deleted 6948376088.jpg (size: 500x197)\n",
      "Deleted 4593649681.jpg (size: 500x198)\n",
      "Deleted 3298573714.jpg (size: 500x222)\n",
      "Deleted 456512643.jpg (size: 500x127)\n",
      "Deleted 379505240.jpg (size: 500x236)\n",
      "Deleted 4701096000.jpg (size: 500x131)\n",
      "Deleted 961664774.jpg (size: 500x244)\n",
      "Deleted 2934837034.jpg (size: 400x248)\n",
      "Deleted 241979236.jpg (size: 500x214)\n",
      "Deleted 6167267993.jpg (size: 500x164)\n",
      "Deleted 6039211337.jpg (size: 500x241)\n",
      "Deleted 2380765956.jpg (size: 283x232)\n",
      "Deleted 5745387160.jpg (size: 500x227)\n",
      "Deleted 4558580445.jpg (size: 500x158)\n",
      "Deleted 2752341621.jpg (size: 500x185)\n",
      "Deleted 3887428186.jpg (size: 252x500)\n",
      "Deleted 3191135894.jpg (size: 500x166)\n",
      "Deleted 6022784716.jpg (size: 500x219)\n",
      "Deleted 6889203488.jpg (size: 500x236)\n",
      "Deleted 7703563796.jpg (size: 500x250)\n",
      "Deleted 6905008788.jpg (size: 288x192)\n",
      "Deleted 4907068336.jpg (size: 500x208)\n",
      "Deleted 353313973.jpg (size: 250x500)\n",
      "Deleted 399054696.jpg (size: 500x206)\n",
      "Deleted 484443289.jpg (size: 500x247)\n",
      "Deleted 3610189629.jpg (size: 500x224)\n",
      "Deleted 2057160636.jpg (size: 500x245)\n",
      "Deleted 1460443444.jpg (size: 500x217)\n",
      "Deleted 2795534813.jpg (size: 500x243)\n",
      "Deleted 5914327.jpg (size: 500x213)\n",
      "Deleted 63787714.jpg (size: 500x176)\n",
      "Deleted 4070112495.jpg (size: 180x240)\n",
      "Deleted 2474615549.jpg (size: 500x250)\n",
      "Deleted 2549452277.jpg (size: 500x198)\n",
      "Deleted 3777870218.jpg (size: 174x500)\n",
      "Deleted 1358398144.jpg (size: 500x185)\n",
      "Deleted 6592762245.jpg (size: 500x204)\n",
      "Deleted 2676184321.jpg (size: 500x250)\n",
      "Deleted 176527896.jpg (size: 500x255)\n",
      "Deleted 85318252.jpg (size: 250x500)\n",
      "Deleted 2114739371.jpg (size: 500x252)\n",
      "Deleted 280899903.jpg (size: 181x500)\n",
      "Deleted 8183107966.jpg (size: 500x226)\n",
      "Deleted 428506573.jpg (size: 255x500)\n",
      "Deleted 3247807215.jpg (size: 500x250)\n",
      "Deleted 3057770908.jpg (size: 245x500)\n",
      "Deleted 102351840.jpg (size: 500x198)\n",
      "Deleted 3030728199.jpg (size: 500x210)\n",
      "Deleted 3051022611.jpg (size: 500x200)\n",
      "Deleted 4525537376.jpg (size: 500x241)\n",
      "Deleted 2931254547.jpg (size: 500x214)\n",
      "Deleted 3730260112.jpg (size: 500x211)\n",
      "Deleted 4447231069.jpg (size: 500x227)\n",
      "Deleted 7703564760.jpg (size: 500x250)\n",
      "Deleted 2393264648.jpg (size: 500x187)\n",
      "Deleted 2871691966.jpg (size: 500x254)\n",
      "Deleted 3516960094.jpg (size: 202x500)\n",
      "Deleted 2281075738.jpg (size: 500x203)\n",
      "Deleted 283091793.jpg (size: 233x500)\n",
      "Deleted 6958305371.jpg (size: 500x194)\n",
      "Deleted 2367318629.jpg (size: 500x224)\n",
      "Deleted 4668251659.jpg (size: 500x218)\n",
      "Deleted 3649307685.jpg (size: 246x500)\n",
      "Deleted 5791274887.jpg (size: 500x216)\n",
      "Deleted 2768021570.jpg (size: 500x177)\n",
      "Deleted 4289389552.jpg (size: 500x112)\n",
      "Deleted 3262760716.jpg (size: 500x198)\n",
      "Deleted 415755815.jpg (size: 500x199)\n",
      "Deleted 4951131390.jpg (size: 500x213)\n",
      "Deleted 3653385929.jpg (size: 500x230)\n",
      "Deleted 3340575518.jpg (size: 500x241)\n",
      "Deleted 5008951909.jpg (size: 500x179)\n",
      "Deleted 4037734292.jpg (size: 500x238)\n",
      "Deleted 404895945.jpg (size: 198x500)\n",
      "Deleted 2573009655.jpg (size: 500x244)\n",
      "Deleted 2351870496.jpg (size: 500x248)\n",
      "Deleted 4730527167.jpg (size: 255x500)\n",
      "Deleted 2315350560.jpg (size: 500x225)\n",
      "Deleted 350670306.jpg (size: 500x179)\n",
      "Deleted 3553476195.jpg (size: 500x250)\n",
      "Deleted 4334697926.jpg (size: 500x241)\n",
      "Deleted 7284553600.jpg (size: 500x250)\n",
      "Deleted 3521201948.jpg (size: 500x253)\n",
      "Deleted 4871633378.jpg (size: 500x245)\n",
      "Deleted 4840394080.jpg (size: 500x250)\n",
      "Deleted 3601533527.jpg (size: 255x500)\n",
      "Deleted 4399663161.jpg (size: 500x253)\n",
      "Deleted 4941007356.jpg (size: 500x250)\n",
      "Deleted 2735290454.jpg (size: 253x500)\n",
      "Deleted 7428316382.jpg (size: 500x252)\n",
      "Deleted 6187182607.jpg (size: 500x247)\n",
      "Deleted 271770120.jpg (size: 500x230)\n",
      "Deleted 3717258171.jpg (size: 500x233)\n",
      "Deleted 4254687302.jpg (size: 237x500)\n",
      "Deleted 8170798749.jpg (size: 500x220)\n",
      "Deleted 2774554310.jpg (size: 200x500)\n",
      "Deleted 182897345.jpg (size: 500x229)\n",
      "Deleted 3229442620.jpg (size: 500x228)\n",
      "Deleted 3321063116.jpg (size: 500x247)\n",
      "Deleted 2975417212.jpg (size: 500x221)\n",
      "Deleted 148819644.jpg (size: 500x231)\n",
      "Deleted 2914737181.jpg (size: 500x183)\n",
      "Deleted 89407459.jpg (size: 500x201)\n",
      "Deleted 260217340.jpg (size: 203x500)\n",
      "Deleted 6811962907.jpg (size: 500x167)\n",
      "Deleted 7052777683.jpg (size: 500x170)\n",
      "Deleted 405051459.jpg (size: 228x500)\n",
      "Deleted 7807617146.jpg (size: 500x199)\n",
      "Deleted 4523659515.jpg (size: 500x245)\n",
      "Deleted 3151492269.jpg (size: 500x251)\n",
      "Deleted 3528902357.jpg (size: 218x500)\n",
      "Deleted 7002845832.jpg (size: 500x247)\n",
      "Deleted 23012579.jpg (size: 250x314)\n",
      "Deleted 6082254728.jpg (size: 500x232)\n",
      "Deleted 4568031502.jpg (size: 500x250)\n",
      "Deleted 7376855592.jpg (size: 500x203)\n",
      "Deleted 23018379.jpg (size: 300x245)\n",
      "Deleted 3214381315.jpg (size: 359x239)\n",
      "Deleted 539744890.jpg (size: 180x500)\n",
      "Deleted 72090503.jpg (size: 500x188)\n",
      "Deleted 2704379125.jpg (size: 500x248)\n",
      "Deleted 7661766322.jpg (size: 500x227)\n",
      "Deleted 2708634088.jpg (size: 500x193)\n",
      "Deleted 2308978137.jpg (size: 482x242)\n",
      "Deleted 4692834620.jpg (size: 247x500)\n",
      "Deleted 7070587325.jpg (size: 500x148)\n",
      "Deleted 4460943467.jpg (size: 500x187)\n",
      "Deleted 2844747252.jpg (size: 500x193)\n",
      "Deleted 7052724381.jpg (size: 500x201)\n",
      "Deleted 3225226381.jpg (size: 500x230)\n",
      "Deleted 110615445.jpg (size: 240x500)\n",
      "Deleted 3457315666.jpg (size: 318x255)\n",
      "Deleted 3358558292.jpg (size: 500x247)\n",
      "Deleted 4548958921.jpg (size: 500x253)\n",
      "Deleted 4459723252.jpg (size: 500x241)\n",
      "Deleted 2481490320.jpg (size: 500x200)\n",
      "Deleted 8859482.jpg (size: 500x255)\n",
      "Deleted 4795241726.jpg (size: 500x250)\n",
      "Deleted 8237901726.jpg (size: 500x215)\n",
      "Deleted 2052202553.jpg (size: 500x253)\n",
      "Deleted 2176364472.jpg (size: 400x245)\n",
      "Deleted 4700639206.jpg (size: 500x230)\n",
      "Deleted 4497283483.jpg (size: 500x254)\n",
      "Deleted 7600282242.jpg (size: 500x240)\n",
      "Deleted 2976946039.jpg (size: 500x250)\n",
      "Deleted 2354829523.jpg (size: 500x246)\n",
      "Deleted 4280376593.jpg (size: 500x246)\n",
      "Deleted 442987551.jpg (size: 320x240)\n",
      "Deleted 3246804978.jpg (size: 500x202)\n",
      "Deleted 7703165508.jpg (size: 500x244)\n",
      "Deleted 842961005.jpg (size: 500x213)\n",
      "Deleted 2301867590.jpg (size: 500x240)\n",
      "Deleted 5311134561.jpg (size: 500x252)\n",
      "Deleted 6928261141.jpg (size: 240x413)\n",
      "Deleted 4963461068.jpg (size: 500x194)\n",
      "Deleted 3210944038.jpg (size: 500x250)\n",
      "Deleted 59032661.jpg (size: 500x246)\n",
      "Deleted 330325191.jpg (size: 500x175)\n",
      "Deleted 1420060020.jpg (size: 251x500)\n",
      "Deleted 6073565104.jpg (size: 500x251)\n",
      "Deleted 2244613488.jpg (size: 500x253)\n",
      "Deleted 4626017432.jpg (size: 500x247)\n",
      "Deleted 2665806290.jpg (size: 244x500)\n",
      "Deleted 4970971349.jpg (size: 239x500)\n",
      "Deleted 4558172302.jpg (size: 500x215)\n",
      "Deleted 6319053071.jpg (size: 500x216)\n",
      "Deleted 739944487.jpg (size: 500x234)\n",
      "Deleted 2282895743.jpg (size: 238x500)\n",
      "Deleted 1788886557.jpg (size: 500x252)\n",
      "Deleted 311196733.jpg (size: 500x253)\n",
      "Deleted 521083170.jpg (size: 500x180)\n",
      "Deleted 2453891449.jpg (size: 360x208)\n",
      "Deleted 614590294.jpg (size: 500x208)\n",
      "Deleted 3713177334.jpg (size: 500x248)\n",
      "Deleted 506412121.jpg (size: 500x197)\n",
      "Deleted 106691539.jpg (size: 320x224)\n",
      "Deleted 2220175999.jpg (size: 500x217)\n",
      "Deleted 317383917.jpg (size: 500x243)\n",
      "Deleted 428168186.jpg (size: 500x247)\n",
      "Deleted 203701114.jpg (size: 320x240)\n",
      "Deleted 3206326389.jpg (size: 500x213)\n",
      "Deleted 2868798.jpg (size: 500x222)\n",
      "Deleted 3264678536.jpg (size: 500x238)\n",
      "Deleted 4930032075.jpg (size: 500x207)\n",
      "Deleted 866841633.jpg (size: 500x244)\n",
      "Deleted 3050114829.jpg (size: 500x151)\n",
      "Deleted 3505568481.jpg (size: 360x240)\n",
      "Deleted 3248676970.jpg (size: 500x255)\n",
      "Deleted 4557686027.jpg (size: 500x250)\n",
      "Deleted 442985422.jpg (size: 320x240)\n",
      "Deleted 176673870.jpg (size: 225x500)\n",
      "Deleted 1341787777.jpg (size: 500x233)\n",
      "Deleted 6893463876.jpg (size: 219x500)\n",
      "Deleted 4829926526.jpg (size: 500x239)\n",
      "Deleted 3211029717.jpg (size: 500x218)\n",
      "Deleted 7710096952.jpg (size: 500x250)\n",
      "Deleted 842960985.jpg (size: 500x216)\n",
      "Deleted 2259203920.jpg (size: 448x231)\n",
      "Deleted 3155365418.jpg (size: 202x500)\n",
      "Deleted 3021318991.jpg (size: 500x209)\n",
      "Deleted 410413536.jpg (size: 500x178)\n",
      "Deleted 2485467050.jpg (size: 500x240)\n",
      "Deleted 3649382413.jpg (size: 244x500)\n",
      "Deleted 3553204471.jpg (size: 500x250)\n",
      "Deleted 103205630.jpg (size: 500x215)\n",
      "Deleted 3030079705.jpg (size: 320x240)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, directory, target_image_size=128):\n",
    "        self.directory = directory\n",
    "        self.filenames = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith(('.jpg', '.png'))]\n",
    "        self.target_image_size = target_image_size\n",
    "        self.size_filter_stats = {'valid': 0, 'invalid': 0}  # Initialize statistics\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        while True:\n",
    "            img_path = self.filenames[idx]\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            try:\n",
    "                img = preprocess(img, target_image_size=self.target_image_size)\n",
    "                img = preprocess_vqgan(img)\n",
    "                self.size_filter_stats['valid'] += 1  # Increment valid count\n",
    "                return img\n",
    "            except ValueError:\n",
    "                self.size_filter_stats['invalid'] += 1  # Increment invalid count\n",
    "                idx = (idx + 1) % len(self.filenames)\n",
    "\n",
    "    def get_size_filter_stats(self):\n",
    "        return self.size_filter_stats\n",
    "\n",
    "def preprocess(img, target_image_size=128):\n",
    "    s = min(img.size)\n",
    "    \n",
    "    if s < target_image_size:\n",
    "        raise ValueError(f'min dim for image {s} < {target_image_size}')\n",
    "        \n",
    "    r = target_image_size / s\n",
    "    s = (round(r * img.size[1]), round(r * img.size[0]))\n",
    "    img = TF.resize(img, s, interpolation=Image.LANCZOS)\n",
    "    img = TF.center_crop(img, output_size=2 * [target_image_size])\n",
    "    img = T.ToTensor()(img)  # Remove torch.unsqueeze\n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_vqgan(x):\n",
    "    '''\n",
    "    Why?\n",
    "    '''\n",
    "    x = 2. * x - 1.\n",
    "    return x\n",
    "def delete_small_images(directory, target_image_size):\n",
    "    \"\"\"\n",
    "    Delete images in the specified directory whose height or width is less than the target image size.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Path to the directory containing the images.\n",
    "        target_image_size (int): The minimum size for both height and width of the images.\n",
    "    \"\"\"\n",
    "    deleted_count = 0\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(('.jpg', '.png')):  # Add other extensions as needed\n",
    "            image_path = os.path.join(directory, filename)\n",
    "            try:\n",
    "                with Image.open(image_path) as img:\n",
    "                    width, height = img.size\n",
    "                    if width < target_image_size or height < target_image_size:\n",
    "                        os.remove(image_path)\n",
    "                        deleted_count += 1\n",
    "                        print(f\"Deleted {filename} (size: {width}x{height})\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {e}\")\n",
    "delete_small_images(image_directory_path, target_image_size=target_image_size)\n",
    "dataset = ImageDataset(image_directory_path, target_image_size=target_image_size)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do the forward pass of VQ-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyk/miniconda3/envs/taming/lib/python3.8/site-packages/torchvision/transforms/functional.py:96: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of valid images: 31461\n",
      "Number of invalid images (too small): 0\n",
      "\n",
      "          Batch size = 31461,\n",
      "          Code dimension = 256,\n",
      "          Embedding height = 16,\n",
      "          Embedding width = 16 \n",
      "          \n"
     ]
    }
   ],
   "source": [
    "all_z_q = []\n",
    "all_indices_unflattened = []\n",
    "all_images = []\n",
    "\n",
    "for img_batch in dataloader:\n",
    "    img_batch = img_batch.to(DEVICE) \n",
    "    # img --> z_q: (B, C, H, W) --> (B, D, h, w)\n",
    "    z_q, indices = model.encode_to_z(img_batch) \n",
    "    B, D, h, w = z_q.shape\n",
    "    assert h * w == indices.shape[1]  # Assert that len(indices)==h*w\n",
    "    \n",
    "    indices_unflattened = indices.reshape(B, h, w)  # (B, h, w)\n",
    "\n",
    "    all_z_q.append(z_q)\n",
    "    all_indices_unflattened.append(indices_unflattened)\n",
    "    # all_images.append(img_batch)\n",
    "    # print(z_q.shape)\n",
    "    \n",
    "    \n",
    "# Concatenate all collected z_q and indices_unflattened tensors\n",
    "all_z_q = torch.cat(all_z_q, dim=0)\n",
    "all_indices_unflattened = torch.cat(all_indices_unflattened, dim=0)\n",
    "# all_images = torch.cat(all_images, dim=0)\n",
    "# Print the statistics\n",
    "stats = dataset.get_size_filter_stats()\n",
    "print(f\"Number of valid images: {stats['valid']}\")\n",
    "print(f\"Number of invalid images (too small): {stats['invalid']}\")\n",
    "\n",
    "\n",
    "# The rest of your code to process the concatenated tensors\n",
    "B, D, h, w = all_z_q.shape\n",
    "print(f'''\n",
    "          Batch size = {B},\n",
    "          Code dimension = {D},\n",
    "          Embedding height = {h},\n",
    "          Embedding width = {w} \n",
    "          ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Codebook shape: torch.Size([1024, 256])\n",
      "Codebook: tensor([[ 0.2046, -0.8589, -0.1892,  ..., -0.1544,  0.8922,  0.2580],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.8011, -1.9389, -0.9249,  ..., -0.7664,  0.5944,  0.6733],\n",
      "        [-0.9394, -0.4141, -1.2881,  ...,  0.3234, -0.6580, -1.4466],\n",
      "        [ 0.0707, -1.0961, -1.6857,  ...,  1.1182,  1.0006, -1.1370]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "D = config.model.params.first_stage_config.params.embed_dim # D := the dimension number of each code e_k in the codebook.\n",
    "K = config.model.params.first_stage_config.params.n_embed # codebook size\n",
    "\n",
    "indices_flattened = all_indices_unflattened.view(-1)  # Shape: (B*h*w,)\n",
    "unique_indices = indices_flattened.unique()\n",
    "    \n",
    "z_q_flattened = all_z_q.permute(0, 2, 3, 1).reshape(-1, D)  # Shape: (B*h*w, D)\n",
    "    \n",
    "# Initialize the codebook (K, D)\n",
    "codebook = torch.zeros((K, D), device=all_z_q.device, dtype=all_z_q.dtype)\n",
    "    \n",
    "# Populate the codebook\n",
    "for idx in unique_indices:\n",
    "    codebook[idx] = z_q_flattened[indices_flattened == idx].mean(dim=0)\n",
    "\n",
    "print(\"Codebook shape:\", codebook.shape)\n",
    "print(\"Codebook:\", codebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Codebook shape: torch.Size([1024, 256])\n",
      "Codebook: tensor([[ 0.2046, -0.8589, -0.1892,  ..., -0.1544,  0.8922,  0.2580],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.8011, -1.9389, -0.9249,  ..., -0.7664,  0.5944,  0.6733],\n",
      "        [-0.9394, -0.4141, -1.2881,  ...,  0.3234, -0.6580, -1.4466],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have the following variables already defined:\n",
    "# z_q: The encoded tensor of shape (B, D, h, w)\n",
    "# indices_unflattened: The indices tensor of shape (B, h, w)\n",
    "# D: The dimension of each code\n",
    "# K: The number of unique codes in the codebook\n",
    "\n",
    "# Step 1: Flatten indices_unflattened to get a 1D array of indices\n",
    "indices_flattened = indices_unflattened.view(-1)  # Shape: (B*h*w,)\n",
    "\n",
    "# Step 2: Get unique indices\n",
    "unique_indices = indices_flattened.unique()\n",
    "\n",
    "# Step 3: Gather the codes from z_q based on unique indices\n",
    "# We need to reshape z_q to (B*h*w, D) to easily gather the codes\n",
    "z_q_flattened = z_q.permute(0, 2, 3, 1).reshape(-1, D)  # Shape: (B*h*w, D)\n",
    "\n",
    "# Initialize the codebook (K, D)\n",
    "codebook = torch.zeros((K, D), device=z_q.device, dtype=z_q.dtype)\n",
    "\n",
    "# Populate the codebook\n",
    "\n",
    "# Fill the codebook with the mean of the vectors corresponding to each unique index.\n",
    "# Creates a boolean mask where elements are True if they match the current idx and False otherwise.\n",
    "# Uses this boolean mask to select all vectors in z_q_flattened that correspond to the current idx.\n",
    "# Computes the mean of the selected vectors along the 0th dimension (which is the batch dimension). This results in a single vector of shape (D,) which represents the average vector for the current idx.\n",
    "for idx in unique_indices:\n",
    "    codebook[idx] = z_q_flattened[indices_flattened == idx].mean(dim=0)\n",
    "\n",
    "print(\"Codebook shape:\", codebook.shape)\n",
    "print(\"Codebook:\", codebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the results\n",
    "Save the feature map (`z_q`), indices, codebook and original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary saved to /home/lyk/Projects/taming-transformers/outputs/result.pth\n"
     ]
    }
   ],
   "source": [
    "res = {\n",
    "  \"z_q\": all_z_q, # (B, D, h, w)\n",
    "  \"indices\": all_indices_unflattened, # (B, h, w)\n",
    "  \"codebook\": codebook, # (K, D)\n",
    "  # \"image\": img_batch, # float, tensor\n",
    "}\n",
    "# Here `B` is the total dataset size.\n",
    "\n",
    "\n",
    "\n",
    "# Save the dictionary to the specified file path\n",
    "torch.save(res, file_path_to_save)\n",
    "\n",
    "print(f\"Dictionary saved to {file_path_to_save}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the results and visualize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dictionary from the file\n",
    "loaded_res = torch.load(file_path_to_save)\n",
    "\n",
    "# Access the images and indices from the loaded dictionary\n",
    "# img_batch = loaded_res[\"image\"]\n",
    "indices = loaded_res[\"indices\"]\n",
    "codebook = loaded_res[\"codebook\"]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to visualize the images\n",
    "# def visualize_images(img_batch):\n",
    "#     \"\"\"\n",
    "#     Visualize all images in the batch.\n",
    "\n",
    "#     Args:\n",
    "#         img_batch (torch.Tensor): Batch tensor of shape (B, C, H, W).\n",
    "#     \"\"\"\n",
    "#     B, C, H, W = img_batch.shape\n",
    "#     img_batch = img_batch.cpu().numpy()  # Move to CPU and convert to numpy\n",
    "\n",
    "#     # If the images are in the format (B, C, H, W), we need to transpose them to (B, H, W, C)\n",
    "#     img_batch = img_batch.transpose(0, 2, 3, 1)\n",
    "\n",
    "#     for i in range(B):\n",
    "#         img = img_batch[i]\n",
    "#         img = (img - img.min()) / (img.max() - img.min())  # Normalize to [0, 1]\n",
    "#         plt.figure(figsize=(6, 6))\n",
    "#         plt.imshow(img)\n",
    "#         plt.title(f'Image {i+1}')\n",
    "#         plt.axis('off')\n",
    "#         plt.show()\n",
    "\n",
    "# # Visualize the first 8 images\n",
    "# visualize_images(img_batch[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualize the histogram of indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unused indices (total 574): [   1    2    3    5    8    9   10   11   14   15   16   17   18   21\n",
      "   22   23   24   25   26   30   33   34   38   41   42   43   45   46\n",
      "   50   51   52   53   59   60   62   64   65   67   68   69   70   71\n",
      "   73   74   75   76   77   78   79   81   85   86   87   88   89   90\n",
      "   91   93   95   97   98   99  100  101  103  104  105  106  108  109\n",
      "  111  113  114  115  116  118  121  123  124  127  130  131  133  135\n",
      "  137  138  139  141  142  145  146  149  151  152  153  155  157  158\n",
      "  159  160  161  162  165  167  169  170  174  177  178  179  180  181\n",
      "  182  183  190  191  194  195  196  197  198  199  201  203  204  205\n",
      "  207  209  211  212  213  214  216  221  223  224  227  228  229  231\n",
      "  232  233  234  235  236  238  239  240  241  242  246  249  251  252\n",
      "  259  262  264  265  266  267  268  269  270  271  272  273  274  275\n",
      "  277  278  279  280  281  282  284  285  286  288  289  290  291  292\n",
      "  293  296  298  300  302  303  305  307  309  310  311  314  317  318\n",
      "  319  321  322  324  326  331  332  336  337  338  340  341  342  344\n",
      "  346  347  348  349  352  353  355  356  357  359  360  361  364  366\n",
      "  367  368  369  372  374  377  378  380  381  383  384  385  386  387\n",
      "  388  390  391  392  394  395  396  397  398  399  401  403  405  408\n",
      "  409  410  411  412  413  415  419  420  421  423  424  427  429  430\n",
      "  431  434  440  441  442  443  445  446  448  449  451  454  456  458\n",
      "  461  462  463  465  470  472  474  475  476  478  480  482  484  486\n",
      "  487  488  490  491  492  495  496  497  502  504  506  507  509  510\n",
      "  511  512  513  516  521  522  524  525  527  529  530  531  532  535\n",
      "  536  538  540  541  543  544  547  549  554  555  557  559  560  561\n",
      "  563  565  566  567  568  569  570  571  573  581  584  592  594  602\n",
      "  603  604  605  606  612  613  615  616  619  620  621  622  625  629\n",
      "  631  637  640  641  643  645  646  647  648  649  650  651  652  654\n",
      "  655  659  665  667  670  673  674  676  677  679  686  687  689  690\n",
      "  691  693  694  696  699  702  703  704  708  709  711  715  716  721\n",
      "  722  723  724  726  727  728  732  733  734  735  736  739  740  743\n",
      "  744  747  750  752  753  754  755  756  757  760  762  763  764  765\n",
      "  766  767  768  770  775  776  777  779  781  782  785  786  791  792\n",
      "  793  795  796  798  799  804  809  814  815  817  818  821  824  825\n",
      "  826  827  829  831  832  833  834  836  838  839  842  843  844  846\n",
      "  847  850  852  853  855  858  859  860  861  863  864  866  867  871\n",
      "  874  876  879  880  882  883  885  886  888  889  890  895  896  897\n",
      "  898  900  902  904  905  907  912  916  917  919  920  923  926  927\n",
      "  930  931  933  934  935  938  939  940  942  943  944  947  948  949\n",
      "  950  951  952  953  954  955  958  959  960  962  963  967  968  971\n",
      "  973  975  978  980  981  983  984  986  987  989  991  995  997  998\n",
      "  999 1000 1001 1003 1004 1005 1009 1010 1011 1013 1014 1015 1016 1020]\n",
      "Shannon entropy: 8.74924007607881\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def calculate_shannon_entropy(probabilities):\n",
    "    \"\"\"\n",
    "    Calculate the Shannon entropy of a probability distribution.\n",
    "    \n",
    "    Args:\n",
    "        probabilities (np.ndarray): Array of probabilities.\n",
    "    \n",
    "    Returns:\n",
    "        float: The Shannon entropy.\n",
    "    \"\"\"\n",
    "    # Filter out zero probabilities to avoid log(0)\n",
    "    probabilities = probabilities[probabilities > 0]\n",
    "    entropy = -np.sum(probabilities * np.log2(probabilities))\n",
    "    return entropy\n",
    "\n",
    "def visualize_indices_frequency_distribution(indices, codebook_size):\n",
    "    \"\"\"\n",
    "    Visualize the distribution of indices as frequencies using a bar plot, sorted by frequency in descending order.\n",
    "\n",
    "    Args:\n",
    "        indices (torch.Tensor): Tensor of shape (B, h, w) containing the indices.\n",
    "        codebook_size (int): The number of codes in the codebook.\n",
    "    \"\"\"\n",
    "    indices_flattened = indices.view(-1).cpu().numpy()\n",
    "    \n",
    "    # Count the occurrences of each index\n",
    "    index_counts = np.bincount(indices_flattened, minlength=codebook_size)\n",
    "    \n",
    "    # Find unused indices\n",
    "    unused_indices = np.where(index_counts == 0)[0]\n",
    "    \n",
    "    if len(unused_indices) > 0:\n",
    "        print(f\"Unused indices (total {len(unused_indices)}): {unused_indices}\")\n",
    "    else:\n",
    "        print(\"All indices are used at least once.\")\n",
    "    \n",
    "    # Calculate probabilities\n",
    "    total_count = np.sum(index_counts)\n",
    "    probabilities = index_counts / total_count\n",
    "    \n",
    "    # Calculate Shannon entropy\n",
    "    entropy = calculate_shannon_entropy(probabilities)\n",
    "    print(f\"Shannon entropy: {entropy}\")\n",
    "    \n",
    "    # Sort indices by frequency in descending order\n",
    "    sorted_indices = np.argsort(index_counts)[::-1]\n",
    "    sorted_counts = index_counts[sorted_indices]\n",
    "    # print(sorted_indices[-1], sorted_counts[-1])\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(range(codebook_size), sorted_counts, color='skyblue', edgecolor='black')\n",
    "    plt.title('Frequency Distribution of Codebook Indices')\n",
    "    plt.xlabel('Index (sorted by frequency)')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    # Optionally, hide x-axis labels if there are too many indices\n",
    "    if codebook_size > 50:  # Adjust the threshold as needed\n",
    "        plt.xticks([])\n",
    "    \n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the probability distribution of indices\n",
    "codebook_size = codebook.shape[0]\n",
    "visualize_indices_frequency_distribution(indices, codebook_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "430.31224261000375"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**8.74924007607881"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " visualize the codebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize the codebook\n",
    "def visualize_codebook(codebook):\n",
    "    \"\"\"\n",
    "    Visualize the codebook as an image grid.\n",
    "\n",
    "    Args:\n",
    "        codebook (torch.Tensor): Tensor of shape (K, D) where K is the number of codes and D is the dimension.\n",
    "    \"\"\"\n",
    "    K, D = codebook.shape\n",
    "    codebook = codebook.cpu().numpy()  # Move to CPU and convert to numpy\n",
    "\n",
    "    # Reshape the codebook to visualize it\n",
    "    codebook_image = codebook.reshape((int(np.sqrt(K)), int(np.sqrt(K)), int(np.sqrt(D)), int(np.sqrt(D))))\n",
    "    codebook_image = codebook_image.transpose(0, 2, 1, 3).reshape(int(np.sqrt(K)) * int(np.sqrt(D)), int(np.sqrt(K)) * int(np.sqrt(D)))\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(codebook_image, cmap='viridis')\n",
    "    plt.title('Codebook Visualization')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the codebook\n",
    "visualize_codebook(codebook)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
