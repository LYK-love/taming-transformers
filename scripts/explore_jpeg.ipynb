{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Get the directory of the current notebook \n",
    "notebook_dir_path = os.path.dirname(os.path.abspath('__file__'))\n",
    "project_path = os.path.join(notebook_dir_path, '..') # Change this\n",
    "\n",
    "sys.path.append(notebook_dir_path)\n",
    "sys.path.append(project_path)\n",
    "\n",
    "from struct import unpack\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import binascii\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython.display import display, Markdown\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_num_of_images = 150  # Example value, set this to the desired number of iterations\n",
    "\n",
    "# image_directory_path = f\"/share/openimage_validation\"\n",
    "image_directory_path = f\"/share/flickr30k_images\"\n",
    "\n",
    "# image_directory_path = f\"{project_path}/data/test_images\"\n",
    "jpeg_data_path_to_save = f\"{project_path}/outputs/jpeg_data_{total_num_of_images}.bin\"\n",
    "metrics_path_to_save = f\"{project_path}/outputs/metrics_{total_num_of_images}.json\"\n",
    "\n",
    "target_image_size = 256  # Optional: Resize all images to (H, W) while H=W\n",
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "marker_mapping = {\n",
    "    0xffd8: \"Start of Image\",\n",
    "    0xfffe: \"Comment\",\n",
    "    0xffe0: \"Application Default Header\",\n",
    "    0xffdb: \"Quantization Table\",\n",
    "    0xffc0: \"Start of Frame\",\n",
    "    0xffc4: \"Define Huffman Table\",\n",
    "    0xffda: \"Start of Scan\",\n",
    "    0xffd9: \"End of Image\"\n",
    "}\n",
    "\n",
    "\n",
    "class JPEG:\n",
    "    def __init__(self, image_file):\n",
    "        with open(image_file, 'rb') as f:\n",
    "            self.img_data = f.read()\n",
    "    \n",
    "    def decode(self):\n",
    "        data = self.img_data\n",
    "        while(True):\n",
    "            marker, = unpack(\">H\", data[0:2])\n",
    "            print(marker_mapping.get(marker))\n",
    "            if marker == 0xffd8: # \"Start of Image\", there is no `length` field after it.\n",
    "                data = data[2:]\n",
    "            elif marker == 0xffd9: # \"End of Image\"\n",
    "                return\n",
    "            elif marker == 0xffda: # If pointing to \"Start of Scan\", jump to the last 2nd byte, i.e., \"End of Image\".\n",
    "                len_start_of_scan_and_image_data = len(data[:2])\n",
    "                print(f\"Length of \\\"Start of Scan\\\" segment (including \\\"Image Data\\\" segment): {len_start_of_scan_and_image_data}\")\n",
    "                \n",
    "                data = data[-2:]\n",
    "            else:\n",
    "                lenchunk, = unpack(\">H\", data[2:4])\n",
    "                print(f\"Length of this segment: {lenchunk}\")\n",
    "                data = data[2+lenchunk:]            \n",
    "            if len(data)==0:\n",
    "                break \n",
    "    def extract_data(self):\n",
    "        '''\n",
    "        Extract the binaries between `0xffd8` and `0xffd9`, i.e., all the data between \"Start of Image\" and \"End of Image\" segments.\n",
    "        '''\n",
    "        data = self.img_data\n",
    "        start_idx = data.find(b'\\xff\\xd8')  # Start of Image\n",
    "        if start_idx == -1:\n",
    "            return None  # Start marker not found\n",
    "\n",
    "        end_idx = data.find(b'\\xff\\xd9', start_idx)  # End of Image\n",
    "        if end_idx == -1:\n",
    "            return None  # End marker not found\n",
    "\n",
    "        start_idx += 2 # Exclute the start marker\n",
    "        length = end_idx - start_idx\n",
    "        return data[start_idx:end_idx], length  # Exclute the end marker.\n",
    "\n",
    "\n",
    "\n",
    "def calculate_ngram_distribution(tokens, N=20):\n",
    "    \"\"\"Calculate the n-gram distribution for a list of tokens.\"\"\"\n",
    "    # Dictionary to count occurrences of each n-gram\n",
    "    ngram_counts = defaultdict(int)\n",
    "    \n",
    "    # Iterate over the token list by stepping through each token\n",
    "    for i in range(len(tokens) - N + 1):  # Adjust loop to ensure enough tokens for the last n-gram\n",
    "        # Extract N consecutive tokens to form an n-gram\n",
    "        ngram = tuple(tokens[i:i+N])\n",
    "        ngram_counts[ngram] += 1\n",
    "    \n",
    "    # Total number of n-grams\n",
    "    total_ngrams = sum(ngram_counts.values())\n",
    "    \n",
    "    # Dictionary to store the probability of each n-gram\n",
    "    ngram_probabilities = {k: v / total_ngrams for k, v in ngram_counts.items()}\n",
    "    \n",
    "    return ngram_probabilities\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_ngram_frequencies(tokens, N=20):\n",
    "    \"\"\"Calculate the n-gram frequencies for a list of tokens.\"\"\"\n",
    "    # Dictionary to count occurrences of each n-gram\n",
    "    ngram_counts = defaultdict(int)\n",
    "    \n",
    "    # Iterate over the token list by stepping through each token\n",
    "    for i in range(len(tokens) - N + 1):  # Adjust loop to ensure enough tokens for the last n-gram\n",
    "        # Extract N consecutive tokens to form an n-gram\n",
    "        ngram = tuple(tokens[i:i+N])\n",
    "        ngram_counts[ngram] += 1\n",
    "    \n",
    "    return ngram_counts\n",
    "\n",
    "def calculate_entropy(probabilities):\n",
    "    entropy = 0\n",
    "    for prob in probabilities.values():\n",
    "        if prob > 0:  # Log of zero is undefined, so we skip those probabilities\n",
    "            entropy -= prob * math.log2(prob)\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def calculate_kl_divergence(ngram_distribution):\n",
    "    \"\"\"Calculate the KL divergence between n-gram distribution and a uniform distribution.\"\"\"\n",
    "    alphabet_size = len(ngram_distribution)\n",
    "    if alphabet_size == 0:\n",
    "        return 0\n",
    "\n",
    "    uniform_prob = 1 / alphabet_size\n",
    "    kl_divergence = sum(\n",
    "        p * math.log2(p / uniform_prob)\n",
    "        for p in ngram_distribution.values()\n",
    "        if p > 0\n",
    "    )\n",
    "    return kl_divergence\n",
    "\n",
    "def calculate_token_frequencies(tokens):\n",
    "    \"\"\"Calculate the frequency of each token in the list.\"\"\"\n",
    "    token_counts = defaultdict(int)\n",
    "    for token in tokens:\n",
    "        token_counts[token] += 1\n",
    "    return token_counts\n",
    "\n",
    "def filter_top_frequent_token(tokens):\n",
    "    \"\"\"Filter out the top-1 frequent token from the token list.\"\"\"\n",
    "    # Calculate token frequencies\n",
    "    token_counts = calculate_token_frequencies(tokens)\n",
    "    \n",
    "    # Identify the most frequent token\n",
    "    most_frequent_token = max(token_counts, key=token_counts.get)\n",
    "    \n",
    "    # Create a new list excluding the most frequent token\n",
    "    filtered_tokens = [token for token in tokens if token != most_frequent_token]\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "def remove_top_ngram(ngram_frequencies, ngram_distribution):\n",
    "    \"\"\"Remove the n-gram with the top frequency/probability and recalculate the distribution.\"\"\"\n",
    "    \n",
    "    # Identify the n-gram with the highest frequency/probability\n",
    "    top_ngram = max(ngram_frequencies, key=ngram_frequencies.get)\n",
    "    \n",
    "    # Remove the top n-gram\n",
    "    ngram_frequencies.pop(top_ngram, None)\n",
    "    \n",
    "    # Recalculate the total n-grams\n",
    "    total_ngrams = sum(ngram_frequencies.values())\n",
    "    \n",
    "    # Recalculate the probabilities\n",
    "    ngram_distribution = {k: v / total_ngrams for k, v in ngram_frequencies.items()}\n",
    "    \n",
    "    return ngram_frequencies, ngram_distribution\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Byte pair encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_byte_pairs(tokens):\n",
    "    \"\"\"Extract all byte pairs from the tokens.\"\"\"\n",
    "    pairs = defaultdict(int)\n",
    "    prev_token = tokens[0]\n",
    "    for token in tokens[1:]:\n",
    "        pairs[(prev_token, token)] += 1\n",
    "        prev_token = token\n",
    "    return pairs\n",
    "\n",
    "def merge_byte_pair(tokens, pair_to_merge, new_token):\n",
    "    \"\"\"Merge the most frequent byte pair in the tokens using a new token.\"\"\"\n",
    "    merged_tokens = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        if i < len(tokens) - 1 and (tokens[i], tokens[i + 1]) == pair_to_merge:\n",
    "            merged_tokens.append(new_token)\n",
    "            i += 2\n",
    "        else:\n",
    "            merged_tokens.append(tokens[i])\n",
    "            i += 1\n",
    "    return merged_tokens\n",
    "\n",
    "def byte_pair_encoding(tokens, num_merges):\n",
    "    \"\"\"Perform BPE on token data.\"\"\"\n",
    "    max_token_value = max(tokens)\n",
    "    current_token_value = max_token_value + 1\n",
    "    merge_operations = []\n",
    "\n",
    "    for _ in range(num_merges):\n",
    "        pairs = get_byte_pairs(tokens)\n",
    "        if not pairs:\n",
    "            break\n",
    "        most_frequent_pair = max(pairs, key=pairs.get)\n",
    "        tokens = merge_byte_pair(tokens, most_frequent_pair, current_token_value)\n",
    "        merge_operations.append((current_token_value, most_frequent_pair))\n",
    "        current_token_value += 1\n",
    "\n",
    "    return tokens, merge_operations\n",
    "\n",
    "def decode_bpe(encoded_tokens, merge_operations):\n",
    "    \"\"\"Decode the BPE encoded tokens using the merge operations.\"\"\"\n",
    "    for new_token, pair in reversed(merge_operations):\n",
    "        decoded_tokens = []\n",
    "        i = 0\n",
    "        while i < len(encoded_tokens):\n",
    "            if encoded_tokens[i] == new_token:\n",
    "                decoded_tokens.extend(pair)\n",
    "                i += 1\n",
    "            else:\n",
    "                decoded_tokens.append(encoded_tokens[i])\n",
    "                i += 1\n",
    "        encoded_tokens = decoded_tokens\n",
    "    return encoded_tokens\n",
    "\n",
    "def compression_ratio(original_data, compressed_data):\n",
    "    \"\"\"Calculate the compression ratio.\"\"\"\n",
    "    return len(compressed_data) / len(original_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_data = byte_pair_encoding(b'FFAAdwdwd212114241423432421431', num_merges=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read binary data from JPEG files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all JPEG or PNG image paths\n",
    "image_paths = [os.path.join(image_directory_path, f) for f in os.listdir(image_directory_path) if f.endswith(('.jpg', '.jpeg', ''))]\n",
    "\n",
    "\n",
    "\n",
    "binaries_and_lengths = []\n",
    "\n",
    "for i, image_path in enumerate(image_paths):\n",
    "    if i >= total_num_of_images:\n",
    "        break\n",
    "    jpeg_image = JPEG(image_path)\n",
    "    data, length = jpeg_image.extract_data()\n",
    "    binaries_and_lengths.append(\n",
    "        {\"data\": data,\n",
    "         \"length\": length}\n",
    "    )\n",
    "    # Optional: print information about each processed image\n",
    "    print(f\"File: {os.path.basename(image_path)}\")\n",
    "    print(f\"Length of data: {length} bytes\")\n",
    "    # print(f\"Data snippet (first 100 bytes or full data if shorter): {data[:100]}\")\n",
    "\n",
    "# Concatenate all binary data for byte pair distribution analysis\n",
    "all_binary_data = b''.join([item['data'] for item in binaries_and_lengths if item['data']])\n",
    "\n",
    "print(f\"Number of images: {len(binaries_and_lengths)}\")\n",
    "print(f\"Size in bytes: {len(all_binary_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BPE\n",
    "\n",
    "# Convert binary data to a list of integers\n",
    "tokens = list(all_binary_data)\n",
    "\n",
    "# Apply BPE on the byte data\n",
    "num_merges = 5  # Adjust this value based on experimentation\n",
    "encoded_tokens, merge_operations = byte_pair_encoding(tokens, num_merges)\n",
    "\n",
    "\n",
    "# Print the result\n",
    "print(f\"Original data: {all_binary_data}\")\n",
    "print(f\"Encoded tokens: {len(encoded_tokens)}\")\n",
    "print(f\"Compression ratio: {compression_ratio(tokens, encoded_tokens):.2f}\")\n",
    "\n",
    "# # Decode the BPE encoded data\n",
    "# decoded_tokens = decode_bpe(encoded_tokens, merge_operations)\n",
    "\n",
    "# # Convert back to binary data\n",
    "# decoded_binary_data = bytes(decoded_tokens)\n",
    "# print(f\"Decoded data: {decoded_binary_data}\")\n",
    "\n",
    "# # Verify the decoding process\n",
    "# assert all_binary_data == decoded_binary_data, \"Decoded data does not match original data!\"\n",
    "\n",
    "# print(\"Decoding successful and data matches the original.\")\n",
    "\n",
    "\n",
    "# encoded_tokens = filter_top_frequent_token(encoded_tokens)\n",
    "# print(f\"Tokens left after filtering: {len(encoded_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterage N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "metrics = []\n",
    "\n",
    "for N in range(1, 21):  # 21 is exclusive, so it iterates up to 20\n",
    "    start_time = time.time()\n",
    "    \n",
    "    ngram_distribution = calculate_ngram_distribution(encoded_tokens, N=N)\n",
    "    ngram_frequencies= calculate_ngram_frequencies(encoded_tokens, N=N)\n",
    "\n",
    "    ngram_distribution, ngram_frequencies = remove_top_ngram(ngram_distribution, ngram_frequencies) # Remove top-1 frequent ngram\n",
    "    # Calculate statistical measures\n",
    "    frequency_values = list(ngram_frequencies.values())\n",
    "    mean_frequency = np.mean(frequency_values)\n",
    "    median_frequency = np.median(frequency_values)\n",
    "\n",
    "    # Calculate entropy\n",
    "    entropy = calculate_entropy(ngram_distribution)\n",
    "    alphabet_size = len(ngram_distribution)\n",
    "    entropy_if_uniform = math.log2(alphabet_size) if alphabet_size > 0 else 0\n",
    "    difference = abs(entropy - entropy_if_uniform)\n",
    "    kl_divergence = calculate_kl_divergence(ngram_distribution)\n",
    "    \n",
    "    \n",
    "    # Sort ngrams by frequencies in descending order\n",
    "    sorted_ngrams_by_freq = sorted(ngram_frequencies.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Find the index where frequency drops to 1\n",
    "    # (assuming there are any n-grams with frequency 1)\n",
    "    index_frequency_one = next((i for i, (_, freq) in enumerate(sorted_ngrams_by_freq) if freq == 1), None)\n",
    "\n",
    "    top_5_freq_ngrams = sorted_ngrams_by_freq[:5]\n",
    "\n",
    "    # print(\"Top-5 n-grams and their frequencies:\")\n",
    "    # for ngram, frequency in top_5_freq_ngrams:\n",
    "    #     print(f\"{ngram}: {frequency}\")\n",
    "        \n",
    "    # # Assuming ngram_frequencies has been computed\n",
    "    # top_ngrams = sorted(ngram_frequencies.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    # # Prepare and print top 5 n-grams with their frequencies\n",
    "    # top_ngrams_formatted = []\n",
    "    # for ngram, frequency in top_ngrams:\n",
    "    #     # Convert the binary n-gram to a hexadecimal string\n",
    "    #     hex_representation = \"0x\" + binascii.hexlify(ngram).decode().upper()\n",
    "    #     # Append the formatted string with frequency to the list\n",
    "    #     top_ngrams_formatted.append(f\"{hex_representation} (Frequency: {frequency})\")\n",
    "        \n",
    "    \n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "\n",
    "    metrics.append({\n",
    "        \"N\": N,\n",
    "        \"alphabet_size\": alphabet_size,\n",
    "        \"frequency_values\": frequency_values,\n",
    "        \"entropy\": entropy,\n",
    "        \"kl_divergence\": kl_divergence,\n",
    "        \"index_frequency_one\": index_frequency_one,\n",
    "        \"execution_time\": execution_time\n",
    "    })\n",
    "    \n",
    "    # Output results\n",
    "    print(f\"N-gram where N={N}\")\n",
    "    print(f\"Alphabet size (unique tokens): {alphabet_size}\")\n",
    "    print(f\"Mean frequency: {mean_frequency:.2f}, Median frequency: {median_frequency}\")\n",
    "    print(f\"The entropy of the token distribution is: {entropy:.4f} bits\")\n",
    "    print(f\"The entropy of the ideal token uniform distribution is: {entropy_if_uniform:.4f} bits\")\n",
    "    print(f\"The KL divergence from uniform distribution: {kl_divergence:.4f} bits\")\n",
    "    print(f\"The difference: {difference:.4f} bits\")\n",
    "\n",
    "    # if index_frequency_one is not None:\n",
    "    #     print(f\"Index after which all n-grams have frequencies of 1: {index_frequency_one}\")\n",
    "    # else:\n",
    "    #     print(\"No n-grams with frequency of 1 found.\")\n",
    "\n",
    "    # # Print all top n-grams\n",
    "    # print(\"Top 5 N-grams and their frequencies:\")\n",
    "    # for ngram in top_ngrams_formatted:\n",
    "    #     print(ngram)\n",
    "    \n",
    "    print(f\"Execution time: {execution_time:.4f} s\")\n",
    "    \n",
    "    print(\"-\" * 50)  # Separator for readability between different N outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save JPEG data and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Save binary data to file\n",
    "with open(jpeg_data_path_to_save, 'wb') as file:\n",
    "    file.write(all_binary_data)\n",
    "    \n",
    "# Save metrics to file\n",
    "with open(metrics_path_to_save, 'w') as file:\n",
    "    json.dump(metrics, file, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the metrics for Ns\n",
    "See `./analyze_jepg_stats.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# Extract values for plotting\n",
    "N_values = [metric['N'] for metric in metrics]\n",
    "entropies = [metric['entropy'] for metric in metrics]\n",
    "kl_divergences = [metric['kl_divergence'] for metric in metrics]\n",
    "index_frequencies = [metric['index_frequency_one'] if metric['index_frequency_one'] is not None else -1 for metric in metrics]\n",
    "alphabet_sizes = [metric['alphabet_size'] for metric in metrics]\n",
    "frequency_values = [metric['frequency_values'] for metric in metrics] if 'frequency_values' in metrics[0] else [[] for _ in metrics]\n",
    "execution_times = [metric['execution_time'] for metric in metrics]\n",
    "\n",
    "# Plotting the metrics\n",
    "plt.figure(figsize=(15, 18))\n",
    "\n",
    "# Plot entropy\n",
    "plt.subplot(6, 1, 1)\n",
    "plt.plot(N_values, entropies, marker='o', linestyle='-', color='b')\n",
    "plt.title('Entropy of tokens (BPEed N-gram) Distributions for Different N')\n",
    "plt.xlabel('N (N-gram size)')\n",
    "plt.ylabel('Entropy (bits)')\n",
    "plt.grid(True)\n",
    "plt.xticks(N_values)  # Set x-ticks to be integers\n",
    "plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x:.4f}'))\n",
    "for i, entropy in enumerate(entropies):\n",
    "    plt.annotate(f'{entropy:.4f}', (N_values[i], entropies[i]), textcoords=\"offset points\", xytext=(0,5), ha='center')\n",
    "\n",
    "# Plot KL divergence\n",
    "plt.subplot(6, 1, 2)\n",
    "plt.plot(N_values, kl_divergences, marker='o', linestyle='-', color='g')\n",
    "plt.title('KL Divergence of tokens (BPEed N-gram) Distributions from Uniform Distributions for Different N')\n",
    "plt.xlabel('N (N-gram size)')\n",
    "plt.ylabel('KL Divergence (bits)')\n",
    "plt.grid(True)\n",
    "plt.xticks(N_values)  # Set x-ticks to be integers\n",
    "plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x:.4f}'))\n",
    "for i, kl_divergence in enumerate(kl_divergences):\n",
    "    plt.annotate(f'{kl_divergence:.4f}', (N_values[i], kl_divergences[i]), textcoords=\"offset points\", xytext=(0,5), ha='center')\n",
    "\n",
    "# Plot index where frequency drops to 1\n",
    "plt.subplot(6, 1, 3)\n",
    "plt.plot(N_values, index_frequencies, marker='o', linestyle='-', color='r')\n",
    "plt.title('Index Frequency One of tokens (BPEed N-gram) Distributions for Different N')\n",
    "plt.xlabel('N (N-gram size)')\n",
    "plt.ylabel('Index Frequency One')\n",
    "plt.grid(True)\n",
    "plt.xticks(N_values)  # Set x-ticks to be integers\n",
    "for i, index in enumerate(index_frequencies):\n",
    "    plt.annotate(f'{index}', (N_values[i], index_frequencies[i]), textcoords=\"offset points\", xytext=(0,5), ha='center')\n",
    "\n",
    "# Plot alphabet size\n",
    "plt.subplot(6, 1, 4)\n",
    "plt.plot(N_values, alphabet_sizes, marker='o', linestyle='-', color='m')\n",
    "plt.title('Alphabet Size of tokens (BPEed N-gram) Distributions for Different N')\n",
    "plt.xlabel('N (N-gram size)')\n",
    "plt.ylabel('Alphabet Size')\n",
    "plt.grid(True)\n",
    "plt.xticks(N_values)  # Set x-ticks to be integers\n",
    "for i, alphabet_size in enumerate(alphabet_sizes):\n",
    "    plt.annotate(f'{alphabet_size}', (N_values[i], alphabet_sizes[i]), textcoords=\"offset points\", xytext=(0,5), ha='center')\n",
    "\n",
    "# Plot frequency values (log scale)\n",
    "plt.subplot(6, 1, 5)\n",
    "for i, freq_vals in enumerate(frequency_values):\n",
    "    plt.plot([N_values[i]] * len(freq_vals), freq_vals, 'bo', alpha=0.5)\n",
    "plt.yscale('log')\n",
    "plt.title('Frequency Values of tokens (BPEed N-gram) Distributions for Different N (Log Scale)')\n",
    "plt.xlabel('N (N-gram size)')\n",
    "plt.ylabel('Frequency Values')\n",
    "plt.grid(True)\n",
    "plt.xticks(N_values)  # Set x-ticks to be integers\n",
    "\n",
    "# Plot execution time\n",
    "plt.subplot(6, 1, 6)\n",
    "plt.plot(N_values, execution_times, marker='o', linestyle='-', color='c')\n",
    "plt.title('Execution Time for Different N')\n",
    "plt.xlabel('N (N-gram size)')\n",
    "plt.ylabel('Execution Time (seconds)')\n",
    "plt.grid(True)\n",
    "plt.xticks(N_values)  # Set x-ticks to be integers\n",
    "plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x:.4f}'))\n",
    "for i, exec_time in enumerate(execution_times):\n",
    "    plt.annotate(f'{exec_time:.4f}', (N_values[i], execution_times[i]), textcoords=\"offset points\", xytext=(0,5), ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Markdown table for the metrics\n",
    "markdown_table = \"| N | Entropy (bits) | KL Divergence (bits) | Index Frequency One | Alphabet Size | Execution Time (seconds) |\\n\"\n",
    "markdown_table += \"|---|----------------|----------------------|----------------------|---------------|-------------------------|\\n\"\n",
    "\n",
    "for metric in metrics:\n",
    "    N = metric['N']\n",
    "    entropy = f\"{metric['entropy']:.4f}\"\n",
    "    kl_divergence = f\"{metric['kl_divergence']:.4f}\"\n",
    "    index_frequency_one = metric['index_frequency_one'] if metric['index_frequency_one'] is not None else \"N/A\"\n",
    "    alphabet_size = metric['alphabet_size']\n",
    "    execution_time = f\"{metric['execution_time']:.4f}\"\n",
    "    \n",
    "    markdown_table += f\"| {N} | {entropy} | {kl_divergence} | {index_frequency_one} | {alphabet_size} | {execution_time} |\\n\"\n",
    "\n",
    "# Render the Markdown table\n",
    "display(Markdown(markdown_table))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taming",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
